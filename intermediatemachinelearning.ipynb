{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "INTERMEDIATE MACHINE LEARNING\n"
      ],
      "metadata": {
        "id": "CokLLBsCTva-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Reading the training and test datasets\n",
        "X_full = pd.read_csv('../input/train.csv', index_col = 'Id')\n",
        "X_test_full = pd.read_csv('../input/test.csv', index_col = 'Id')\n",
        "\n",
        "# Defining the target variable (SalePrice) and features for training\n",
        "y = X_full.SalePrice\n",
        "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
        "X = X_full[features].copy()\n",
        "X_test= X_test_full[features].copy()\n",
        "\n",
        "# Splitting the dataset into training and validation sets (80% training, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 0)\n"
      ],
      "metadata": {
        "id": "Tjfwuc4DmMUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Defining different Random Forest models with varying hyperparameters\n",
        "model_1 = RandomForestRegressor(n_estimators=50, random_state=0)\n",
        "model_2 = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "model_3 = RandomForestRegressor(n_estimators=100, criterion='absulate_error', random_state=0)\n",
        "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20, random_state=0)\n",
        "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7, random_state=0)\n",
        "models = [model_1, model_2, model_3, model_4, model_5]"
      ],
      "metadata": {
        "id": "5KM5qe3FmMb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Function to evaluate a model using Mean Absolute Error (MAE)\n",
        "def score_model(model, X_t=X_train, X_v=X_valid, y_t=y_train, y_v=y_valid):\n",
        "    model.fit(X_t, y_t)\n",
        "    preds = model.predict(X_v)\n",
        "    return mean_absolute_error(y_v, preds)\n",
        "\n",
        "# Loop over the models starting from index 9 (10th model)\n",
        "for i in range(9, len(models)):\n",
        "  mae = score_model(models[i])\n",
        "  print(models[i])\n",
        "  print(\"Model %d MAE: %d\" % (i+1, mae))\n"
      ],
      "metadata": {
        "id": "zBUVOQ8LpH1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MISSING VALUES"
      ],
      "metadata": {
        "id": "g51UxbJ5rWtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"dansbecker/melbourne-housing-snapshot\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XXF_eGHpH5W",
        "outputId": "7f25db47-759b-4451-e159-fb196369cf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/dansbecker/melbourne-housing-snapshot?dataset_version_number=5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 451k/451k [00:00<00:00, 846kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/dansbecker/melbourne-housing-snapshot/versions/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#/content/melb_data.csv\n",
        "\n",
        "# Read the dataset and separate target and features\n",
        "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
        "y = data.Price\n",
        "melb_predictors = data.drop(['Price'], axis=1)\n",
        "# Select only numerical features for training\n",
        "X = melb_predictors.select_dtypes(exculde = ['object'])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "collapsed": true,
        "id": "0no9bfTCpH8-",
        "outputId": "bc93bd32-f6d8-45cb-a08b-6c9fb257f956"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/melbourne-housing-snapshot/melb_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-878c5bb60a7d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/melbourne-housing-snapshot/melb_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmelb_predictors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/melbourne-housing-snapshot/melb_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
        "  model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
        "  model.fit(X_train, y_train)\n",
        "  preds = model.predict(X_valid)\n",
        "  return mean_absolute_error(y_valid, preds)"
      ],
      "metadata": {
        "id": "6qmd76kzpIFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 1: Drop columns with missing values\n",
        "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
        "\n",
        "# Drop columns with missing values from the training and validation sets\n",
        "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
        "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
        "\n",
        "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
        "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))"
      ],
      "metadata": {
        "id": "8AK0OWANtFXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create an imputer object to fill missing values\n",
        "my_imputer = SimpleImputer()\n",
        "# Impute missing values in the training and validation datasets\n",
        "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
        "imputed_X_valid = pd.Dataframe(my_imputer.transform(X_valid))\n",
        "\n",
        "# Assign the original column names back to the imputed datasets\n",
        "imputed_X_train.columns = X_train.columns\n",
        "imputed_X_valid.columns = X_valid.columns\n",
        "\n",
        "print(\"MAE from Approach 2 (Imputation):\")\n",
        "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
      ],
      "metadata": {
        "id": "4B31nZ-ntFaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extention to imputation\n",
        "# Approach 3: Imputation with a new feature indicating if a value was missing\n",
        "X_train_plus = X_valid.copy()\n",
        "X_valid_plus = X_valid.copy()\n",
        "\n",
        "# For each column with missing values, add a new column that indicates if the value was missing\n",
        "for col in cols_with_missing:\n",
        "  X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
        "  X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
        "\n",
        "# Impute the missing values again in the training and validation datasets\n",
        "  my_imputer = SimpleImputer()\n",
        "  imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
        "  imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
        "\n",
        "# Assign the original column names back to the imputed datasets\n",
        "  imputed_X_train_plus.columns = X_train_plus.columns\n",
        "  imputed_X_valid_plus.columns = X_valid_plus.columns\n",
        "\n",
        "  print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
        "  print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))\n",
        "#"
      ],
      "metadata": {
        "id": "SegANlvZtFd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "\n",
        "missing_val_count_by_column = (X_train.isnull().sum())\n",
        "print(missing_val_count_by_column[missing_val_count_by_column > 0])"
      ],
      "metadata": {
        "id": "SdKwhSDdyaMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#categorial variables\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
        "\n",
        "# Define target and predictors\n",
        "y = data.Price\n",
        "X = data.drop(['Price'], axis=1)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "\n",
        "# Identify columns with missing values\n",
        "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()]\n",
        "\n",
        "# Drop columns with missing values\n",
        "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
        "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
        "\n",
        "# Identify low cardinality categorical columns (columns with fewer than 10 unique values)\n",
        "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == 'object']\n",
        "\n",
        "# Identify numerical columns\n",
        "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Combine low cardinality columns and numerical columns for feature selection\n",
        "my_cols = low_cardinality_cols + numerical_cols\n",
        "X_train = X_train_full[my_cols].copy()\n",
        "X_valid = X_valid_full[my_cols].copy()\n",
        "\n",
        "# Identify categorical variables\n",
        "s = (X_train.dtypes == 'object')\n",
        "object_cols = list(s[s].index)\n",
        "\n",
        "# Print the list of categorical variables\n",
        "print(\"Categorical variables:\")\n",
        "print(object_cols)\n",
        "\n",
        "# Importing RandomForestRegressor and mean_absolute_error for model evaluation\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Function to evaluate a model with MAE\n",
        "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_valid)\n",
        "    return mean_absolute_error(y_valid, preds)\n",
        "\n",
        "# Approach 1: Drop categorical variables\n",
        "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
        "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
        "\n",
        "# Print MAE for the approach where categorical variables are dropped\n",
        "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
        "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
      ],
      "metadata": {
        "id": "SXmgGq2RyaWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Function to evaluate a model with MAE\n",
        "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
        "  model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "  model.fit(X_train, y_train)\n",
        "  preds = model.predict(X_valid)\n",
        "  return mean_absolute_error(y_valid, preds)\n"
      ],
      "metadata": {
        "id": "u4orQog4yaeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop categorial variables\n",
        "# Approach 1: Drop categorical variables\n",
        "drop_X_train = X_train.select.dtypes(exclude=['object'])\n",
        "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
        "\n",
        "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
        "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n"
      ],
      "metadata": {
        "id": "kvKLNdqPyar-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ordinal encoding\n",
        "# Approach 2: Ordinal Encoding for categorical variables\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Copy the data for encoding\n",
        "label_X_train = X_train.copy()\n",
        "label_X_valid = X_valid.copy()\n",
        "\n",
        "# Apply Ordinal Encoding to categorical columns\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
        "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
        "\n",
        "print(\"MAE from Approach 2 (Ordinal Encoding):\")\n",
        "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n"
      ],
      "metadata": {
        "id": "Wdp9sjtCB00I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach 3: One-Hot Encoding for categorical variables\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Apply One-Hot Encoding to categorical columns\n",
        "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
        "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
        "\n",
        "# Set the index of the new encoded columns to match the original data\n",
        "OH_cols_train.index = X_train.index\n",
        "OH_cols_valid.index = X_valid.index\n",
        "\n",
        "# Drop categorical columns and concatenate the One-Hot Encoded columns\n",
        "num_X_train = X_train.drop(object_cols, axis=1)\n",
        "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
        "\n",
        "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
        "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
        "\n",
        "# Convert column names to strings for consistency\n",
        "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
        "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
        "\n",
        "# Print MAE for the approach using One-Hot Encoding\n",
        "print(\"MAE from Approach 3 (One-Hot Encoding):\")\n",
        "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
      ],
      "metadata": {
        "id": "V7au4N-bB05H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PIPELINES"
      ],
      "metadata": {
        "id": "cBirKUGAR0JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the dataset again\n",
        "data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')\n",
        "\n",
        "# Define target and predictors\n",
        "y = data.Price\n",
        "X = data.drop(['Price'], axis=1)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train_full, X_valid_full, y_train_full, y_valid_full = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorial_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == 'object']\n",
        "numeriacal_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
        "\n",
        "# Select columns for feature set\n",
        "my_cols = categorial_cols + numeriacal_cols\n",
        "X_train = X_train_full[my_cols].copy()\n",
        "X_valid = X_valid_full[my_cols].copy()\n"
      ],
      "metadata": {
        "id": "ZGlCYTL8R43f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create transformer for numerical data (constant imputation)\n",
        "numerical_transformer = SimpleImputer(strategy='constant')\n",
        "\n",
        "# Create transformer for categorical data (most frequent imputation and One-Hot Encoding)\n",
        "categorial_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine both transformers into a preprocessor using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols), ('cat', categorial_transformer, categorial_cols)])\n"
      ],
      "metadata": {
        "id": "2ysUqLZwTpSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=0)\n"
      ],
      "metadata": {
        "id": "MiFmJtvtTpVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Create a pipeline that first processes the data and then fits the model\n",
        "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "my_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation data\n",
        "preds = my_pipeline.predict(X_valid)\n",
        "\n",
        "# Calculate and print the MAE for the pipeline model\n",
        "score = mean_absolute_error(y_valid, preds)\n",
        "print('MAE:', score)"
      ],
      "metadata": {
        "id": "gu0XcGnETpXh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}